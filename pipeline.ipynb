{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BvuOYFERGq2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install keybert\n",
        "%pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BV8rtQOpkYAk"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import random\n",
        "from keybert import KeyBERT\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set k (number of non-base patents per row)\n",
        "PATENT_COUNT_PER_ROW = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qyc_7UKkmgR"
      },
      "outputs": [],
      "source": [
        "def get_base_patents(query_terms):\n",
        "  \"\"\"Query USPTO Bulk Data API for base patent (patent application) metadata based on keywords or phrases.\n",
        "\n",
        "    ## Parameters:\n",
        "    query_terms (list): List of keywords and phrases to query for.\n",
        "\n",
        "    ## Returns:\n",
        "    df (DataFrame): DataFrame with application number, publication number, abstract, and claims of retrieved base patents. \n",
        "  \"\"\"\n",
        "  \n",
        "  df = pd.DataFrame()\n",
        "  for query_term in query_terms:\n",
        "    start = 0\n",
        "    # Format query for URL\n",
        "    query_term = '%22' + query_term + '%22'\n",
        "    query_term.replace(' ', '%20')\n",
        "\n",
        "    # Query USPTO Bulk Data API initially to ensure valid query (if result_count > 0)\n",
        "    bulk_data_api_url = f\"https://developer.uspto.gov/ibd-api/v1/application/publications?searchText={query_term}&start={start}&largeTextSearchFlag=Y\"\n",
        "    bulk_search = requests.get(bulk_data_api_url).json()\n",
        "\n",
        "    # Get all query results\n",
        "    result_count = bulk_search['recordTotalQuantity']\n",
        "    l = []\n",
        "    while start < result_count:\n",
        "      # Query USPTO Bulk Data API for data\n",
        "      bulk_data_api_url = f\"https://developer.uspto.gov/ibd-api/v1/application/publications?searchText={query_term}&start={start}&largeTextSearchFlag=Y\"\n",
        "      bulk_search = requests.get(bulk_data_api_url).json()\n",
        "      bulk_search_results = bulk_search['results']\n",
        "\n",
        "      # Extract relevant patent metadata from results\n",
        "      for result in bulk_search_results:\n",
        "        d = {}\n",
        "        d['App_Number'] = result['patentApplicationNumber']\n",
        "        d['Base_Pub_Number'] = result['publicationDocumentIdentifier']\n",
        "        d['Base_Abstract'] = result['abstractText'][0]\n",
        "\n",
        "        try:\n",
        "          d['Base_Claims'] = result['claimText'][0]\n",
        "        except:\n",
        "          d['Base_Claims'] = result['claimText']\n",
        "\n",
        "        l.append(d)\n",
        "\n",
        "      # Move to next page of query results\n",
        "      start += 100\n",
        "\n",
        "      # Cap base patent count at 10000 due to performance issues\n",
        "      if start > 10000:\n",
        "        break\n",
        "\n",
        "    temp_df = pd.DataFrame(l)\n",
        "    # Drop rows with null claims\n",
        "    temp_df = temp_df[temp_df['Base_Claims'].notnull()]\n",
        "    df = pd.concat([df, temp_df])\n",
        "\n",
        "  df.reset_index(drop=True, inplace=True)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcm3yJYdJfja"
      },
      "outputs": [],
      "source": [
        "### Helper functions and entities for publication number extraction ###\n",
        "\n",
        "# Load T5 model for query number extraction\n",
        "text2text_generator = pipeline(\"text2text-generation\")\n",
        "\n",
        "# Define common country and kind codes for publication numbers\n",
        "# TODO: Add more to reduce data loss?\n",
        "COMMON_COUNTRY_CODES = ['US', 'JP', 'EP', 'WO', 'CN']\n",
        "COMMON_KIND_CODES = ['A', 'A1', 'A2', 'B1', 'B2']\n",
        "\n",
        "def clean_pub_num(n, debug=False):\n",
        "  \"\"\"Clean extracted publication number to match typical format.\n",
        "\n",
        "    ## Parameters:\n",
        "    n (str): Extracted publication number string.\n",
        "    debug (bool): Flag denoting whether or not to print logical errors.\n",
        "\n",
        "    ## Returns:\n",
        "    temp_n (str): Cleaned publication number string.\n",
        "  \"\"\"\n",
        "\n",
        "  # Remove redundant characters\n",
        "  n = n.replace('/', '')\n",
        "  n = n.replace(',', '')\n",
        "  n = n.replace('-', '')\n",
        "\n",
        "  # Split on spaces\n",
        "  spl_n = n.split(' ')\n",
        "  temp_n = ''\n",
        "  for spl in spl_n:\n",
        "    # Check if chunk is prefix, suffix, or publication number body\n",
        "    if spl in COMMON_COUNTRY_CODES or spl.isnumeric() or spl in COMMON_KIND_CODES:\n",
        "      temp_n += spl\n",
        "    # Check if chunk is publication number body with suffix attached\n",
        "    elif (spl[:-1].isnumeric() and spl[-1] in COMMON_KIND_CODES) or (spl[:-2].isnumeric() and spl[-2:] in COMMON_KIND_CODES):\n",
        "      temp_n += spl\n",
        "\n",
        "  # Identify valid publication numbers based on common lengths\n",
        "  if len(temp_n) < 7 or len(temp_n) > 15:\n",
        "    if debug:\n",
        "      print(temp_n)\n",
        "    return np.nan\n",
        "  else:\n",
        "    return temp_n\n",
        "\n",
        "def clean_whitespace(s):\n",
        "  \"\"\"Eliminate common examples of whitespace from string.\n",
        "\n",
        "    ## Parameters:\n",
        "    s (str): String to remove whitespace from.\n",
        "\n",
        "    ## Returns:\n",
        "    s (str): String with whitespace removed.\n",
        "  \"\"\"\n",
        "\n",
        "  s = s.replace('  \\n', '')\n",
        "  s = s.replace('\\n', '')\n",
        "  s = s.replace('  ', ' ')\n",
        "  return s\n",
        "\n",
        "def is_valid_char(c):\n",
        "  \"\"\"Eliminate non-standard (non-English) unicode characters from string.\n",
        "\n",
        "    ## Parameters:\n",
        "    c (char): Character to check validity of.\n",
        "\n",
        "    ## Returns:\n",
        "    bool (bool): Validity status of character.\n",
        "  \"\"\"\n",
        "\n",
        "  uni_c = ord(c)\n",
        "  return uni_c > 31 and uni_c < 127\n",
        "\n",
        "def scrape_google_patents(pub_num, debug=False):\n",
        "  \"\"\"Query Google Patents based on extracted publication number and web scrape corresponding claims.\n",
        "\n",
        "    ## Parameters:\n",
        "    pub_num (str): Cleaned publication number cited in USPTO office action rejection.\n",
        "    debug (bool): Flag denoting whether or not to print logical errors.\n",
        "\n",
        "    ## Returns:\n",
        "    temp_num (DataFrame): Potentially modified publication number with added prefix.\n",
        "    claims (str): Rejection claims scraped from Google Patents.\n",
        "  \"\"\"\n",
        "\n",
        "  # Add no prefix in case it is already part of extracted publication number\n",
        "  country_codes = [''] + COMMON_COUNTRY_CODES\n",
        "  for country_code in country_codes:\n",
        "      # Try publication number with added prefix\n",
        "      temp_num = country_code + pub_num\n",
        "      url = f\"https://patents.google.com/patent/{temp_num}/en\"\n",
        "      page = requests.get(url)\n",
        "\n",
        "      if page.status_code == 404:\n",
        "        continue\n",
        "      else:\n",
        "        parser = BeautifulSoup(page.content, \"html.parser\")\n",
        "        try:\n",
        "          # Find claims text\n",
        "          claims = parser.find('section', itemprop='claims').text\n",
        "          # Clean text\n",
        "          claims = clean_whitespace(''.join(filter(is_valid_char, claims)))\n",
        "          return temp_num, claims[claims.find(')')+1:].strip()\n",
        "        except:\n",
        "          if debug:\n",
        "            print(pub_num)\n",
        "          return np.nan, np.nan\n",
        "\n",
        "  if debug:\n",
        "    print(pub_num)\n",
        "  return np.nan, np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCqys5mQlCoU"
      },
      "outputs": [],
      "source": [
        "def get_positive_samples(df):\n",
        "  \"\"\"Query USPTO Office Action API with base patent application numbers. Extract cited publication numbers from 102 rejection\n",
        "    text and scrape Google Patents for corresponding claims (positive samples).\n",
        "\n",
        "    ## Parameters:\n",
        "    df (DataFrame): DataFrame with base patent metadata.\n",
        "\n",
        "    ## Returns:\n",
        "    df (DataFrame): DataFrame with added rejection metadata.\n",
        "  \"\"\"\n",
        "  \n",
        "  # Set up USPTO Office Action API query\n",
        "  oa_api_url = \"https://developer.uspto.gov/ds-api/oa_actions/v1/records\"\n",
        "  headers = {'Content-Type': 'application/x-www-form-urlencoded', 'Accept': 'application/json'}\n",
        "\n",
        "  for i, r in df.iterrows():\n",
        "    # Format query for API call\n",
        "    app_num = f\"{r['App_Number'][2:]}\"\n",
        "    app_num = '\"' + app_num + '\"'\n",
        "    # Query USPTO Office Action API for data\n",
        "    data = {'criteria': f'patentApplicationNumber:{app_num}', 'start': '0', 'rows': '100'}\n",
        "    oa_search = requests.post(oa_api_url, headers=headers, data=data).json()\n",
        "    oa_response = oa_search['response']\n",
        "\n",
        "    if oa_response['numFound'] > 0:\n",
        "      j = 1\n",
        "      rej_nums = []\n",
        "      for oa in oa_response['docs']:\n",
        "        # Access 102 rejection text, if applicable\n",
        "        if 'sections.section102RejectionText' in oa.keys():\n",
        "          rej_text = oa['sections.section102RejectionText']\n",
        "          if rej_text == None:\n",
        "            continue\n",
        "\n",
        "          rej_text = rej_text[0][:250]\n",
        "          # Pass rejection text to T5 model for cited publication number extraction\n",
        "          extract_num = text2text_generator(f\"question: What is the anticpated patent's publication number with country and kind code? context: {rej_text}\")[0]['generated_text']\n",
        "          # Format extracted publication number\n",
        "          rej_num = clean_pub_num(extract_num, debug=False)\n",
        "          if rej_num is not np.nan and rej_num not in rej_nums:\n",
        "            # Web scrape Google Patents for claims\n",
        "            scrape_res = scrape_google_patents(rej_num, debug=True)\n",
        "            if scrape_res[0] is not np.nan:\n",
        "              df.loc[i, f'Rej_Pub_Number_{j}'], df.loc[i, f'Rej_Claims_{j}'] = scrape_res\n",
        "              j += 1\n",
        "              rej_nums.append(rej_num)\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ct3luJyZGAH0"
      },
      "outputs": [],
      "source": [
        "### Helper functions for negative sample acquisition ###\n",
        "\n",
        "# Initialize keyword extraction model\n",
        "kw_model = KeyBERT()\n",
        "\n",
        "def get_tot_result_count(term):\n",
        "  \"\"\"Determine number of patents with specified keyword or phrase.\n",
        "\n",
        "    ## Parameters:\n",
        "    term (str): String to check result count for.\n",
        "\n",
        "    ## Returns:\n",
        "    tot_result_count (int): Result count for string.\n",
        "  \"\"\"\n",
        "\n",
        "  # Format query for URL\n",
        "  query_term = '%22' + term.lower() + '%22'\n",
        "  query_term.replace(' ', '%20')\n",
        "\n",
        "  bulk_data_api_url = f\"https://developer.uspto.gov/ibd-api/v1/application/publications?searchText={query_term}&rows=1&largeTextSearchFlag=Y\"\n",
        "  keyword_search = requests.get(bulk_data_api_url).json()\n",
        "  tot_result_count = keyword_search['recordTotalQuantity']\n",
        "\n",
        "  return tot_result_count\n",
        "\n",
        "def allocate_term_results(top_terms, tgt_patent_count, debug=False):\n",
        "  \"\"\"Allocate number of results to pull for each keyword or phrase based on availability. Shift per term result count\n",
        "     dynamically in order to guarantee k results per row. \n",
        "\n",
        "    ## Parameters:\n",
        "    top_terms (DataFrame): List of keywords to allocate result counts to.\n",
        "    tgt_patent_count (int): Number of patents per row minus number of rejection patents already obtained for row.\n",
        "    debug (bool): Flag denoting whether or not to print logical errors.\n",
        "\n",
        "    ## Returns:\n",
        "    term_result_alloc_dict (dict): Dictionary with terms as keys and number of results to pull per term as values.\n",
        "  \"\"\"\n",
        "\n",
        "  # Allocate number of results needed evenly per term to start\n",
        "  even_num_results = tgt_patent_count // len(top_terms)\n",
        "  # Get total available result count per term and format as dict\n",
        "  term_result_alloc_dict = {term: [get_tot_result_count(term), even_num_results] for term in top_terms}\n",
        "  # Add remainder of even distribution to first term\n",
        "  term_result_alloc_dict[top_terms[0]][1] += tgt_patent_count % len(top_terms)\n",
        "\n",
        "  # Determine term to default to when allocations exceeds availability for any given term\n",
        "  master_key = 0\n",
        "  # Store terms that lack enough available results\n",
        "  invalid_keys = []\n",
        "  for i, (tot_result_count, num_results) in enumerate(term_result_alloc_dict.values()):\n",
        "    if debug:\n",
        "      print(tot_result_count, num_results)\n",
        "      print(term_result_alloc_dict)\n",
        "\n",
        "    # Check if availability can support allocation\n",
        "    # Build in 2x buffer for allocation count in case of occasional null claims\n",
        "    if tot_result_count <= 2*num_results:\n",
        "      # Test if master key can support allocation of additional terms \n",
        "      temp_num_results = term_result_alloc_dict[top_terms[master_key]][1] + num_results\n",
        "      temp_tot_result = term_result_alloc_dict[top_terms[master_key]][0]\n",
        "      while temp_tot_result <= 2*temp_num_results:\n",
        "        # Change master key if new allocation exceeds availablity\n",
        "        master_key += 1\n",
        "        if master_key == len(top_terms):\n",
        "          # Return empty dict when all terms fail to support total allocation\n",
        "          return dict()\n",
        "        temp_num_results = term_result_alloc_dict[top_terms[master_key]][1] + num_results\n",
        "        temp_tot_result = term_result_alloc_dict[top_terms[master_key]][0]\n",
        "\n",
        "      # Shift allocation of results away from invalid key to master key\n",
        "      term_result_alloc_dict[top_terms[master_key]][1] = temp_num_results\n",
        "      # Mark key as invalid if it cannot support allocation\n",
        "      invalid_keys.append(i)\n",
        "\n",
        "  # Remove invalid keys\n",
        "  for key in invalid_keys:\n",
        "    del term_result_alloc_dict[top_terms[key]]\n",
        "\n",
        "  return term_result_alloc_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G05qLRUtODO7"
      },
      "outputs": [],
      "source": [
        "def get_negative_samples(df, verbose=True):\n",
        "  \"\"\"Extract keywords from base patent abstracts and use to query USPTO Bulk Data API for claims of patents relevant \n",
        "     to base patents (negative samples).\n",
        "  \n",
        "    ## Parameters:\n",
        "    df (DataFrame): DataFrame with base patent and rejection metadata.\n",
        "    verbose (bool): Flag denoting whether or not to print progress markers and rows without k patents.\n",
        "\n",
        "    ## Returns:\n",
        "    df (DataFrame): DataFrame with added non-rejection, relevant patent metadata.\n",
        "  \"\"\"\n",
        "\n",
        "  for i, r in df.iterrows():\n",
        "    # Extract keywords and phrases\n",
        "    top_terms = [term for term, score in kw_model.extract_keywords(r['Base_Abstract'], keyphrase_ngram_range=(1, 2))]\n",
        "    # Determine number of negative patents needed to hit k patents per row\n",
        "    rej_count = len(rej_cols) - r[rej_cols].isna().sum()\n",
        "    tgt_patent_count = PATENT_COUNT_PER_ROW - rej_count\n",
        "\n",
        "    # Allocate result counts per term\n",
        "    term_result_alloc_dict = allocate_term_results(top_terms, tgt_patent_count)\n",
        "\n",
        "    l = []\n",
        "    for term, (tot_result_count, num_results) in term_result_alloc_dict.items():\n",
        "      # Format query for URL\n",
        "      term = '%22' + term.lower() + '%22'\n",
        "      term.replace(' ', '%20')\n",
        "\n",
        "      # Choose random starting point to avoid repeat data\n",
        "      start = random.randint(0, (tot_result_count - 2*num_results - 1))\n",
        "      # Query USPTO Bulk Data API for data\n",
        "      bulk_data_api_url = f\"https://developer.uspto.gov/ibd-api/v1/application/publications?searchText={term}&start={start}&largeTextSearchFlag=Y\"\n",
        "      keyword_search = requests.get(bulk_data_api_url).json()\n",
        "\n",
        "      keyword_search_results = keyword_search['results']\n",
        "      j = 0\n",
        "      for result in keyword_search_results:\n",
        "        try:\n",
        "          # Ignore result if duplicate result for row\n",
        "          pub_num = result['publicationDocumentIdentifier']\n",
        "          if pub_num == r['Base_Pub_Number'] or pub_num in l:\n",
        "            continue\n",
        "          \n",
        "          # Obtain claims text\n",
        "          claims = result['claimText'][0]\n",
        "          l.extend([pub_num, claims])\n",
        "          j += 1\n",
        "          if j == num_results:\n",
        "            break\n",
        "        except:\n",
        "          continue\n",
        "\n",
        "      # Alert upon failure to acquire enough negative samples\n",
        "      if verbose and j != num_results:\n",
        "        print(f\"Retrieval error at row {i}\")\n",
        "\n",
        "      # Set appropriate columns as null to ensure exactly k patents per row\n",
        "      if len(l)/2 == tgt_patent_count:\n",
        "        if len(l)/2 != PATENT_COUNT_PER_ROW:\n",
        "          diff = (PATENT_COUNT_PER_ROW * 2) - len(l)\n",
        "          l.extend([np.nan]*diff)\n",
        "        df.loc[i, non_rej_cols] = l\n",
        "    \n",
        "    # Show completion progress\n",
        "    if verbose:\n",
        "      print(i)\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = get_base_patents(['artificial intelligence'])\n",
        "\n",
        "# Store positive and negative sample data column names\n",
        "rej_cols = [col for col in list(df.columns) if col[:-1] == 'Rej_Claims_']\n",
        "non_rej_cols = []\n",
        "for i in range(1, PATENT_COUNT_PER_ROW+1):\n",
        "  non_rej_cols.extend([f'Pub_Number_{i}', f'Claims_{i}'])\n",
        "\n",
        "df = get_positive_samples(df)\n",
        "df = get_negative_samples(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yE-Pg9IDsV12"
      },
      "outputs": [],
      "source": [
        "df.to_csv('dataset.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
